{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nazalan/cyber-blotto-rl/blob/main/Blotto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzjILrY-Noc2"
      },
      "source": [
        "EPSS download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_G0yJlaPyAS"
      },
      "outputs": [],
      "source": [
        "import os, gzip, shutil, calendar, csv\n",
        "from pathlib import Path\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "# --- Configuration ---\n",
        "YEAR  = 2025\n",
        "MONTH = 8   # 1, 5, and 8 are used in the thesis experiments\n",
        "\n",
        "BASE_DIR   = Path(\"/content/august\")\n",
        "EPSS_DIR   = BASE_DIR / \"epss\"\n",
        "EPSS_CLEAN = BASE_DIR / \"epss_clean\"\n",
        "\n",
        "UA = {\"User-Agent\": \"Mozilla/5.0 (Colab TDK downloader)\"}\n",
        "\n",
        "\n",
        "# --- Directory setup ---\n",
        "def ensure_dirs():\n",
        "    \"\"\"Create working directories if they don’t exist.\"\"\"\n",
        "    for d in (EPSS_DIR, EPSS_CLEAN):\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- HTTP utilities ---\n",
        "def http_download(url: str, dest: Path, headers: dict | None = None) -> None:\n",
        "    \"\"\"Download a file from a URL with optional headers.\"\"\"\n",
        "    req = Request(url, headers=headers or UA)\n",
        "    with urlopen(req, timeout=120) as resp, open(dest, \"wb\") as f:\n",
        "        shutil.copyfileobj(resp, f)\n",
        "\n",
        "\n",
        "def is_gzip(path: Path) -> bool:\n",
        "    \"\"\"Check whether a file is a valid gzip archive.\"\"\"\n",
        "    try:\n",
        "        with open(path, \"rb\") as fh:\n",
        "            return fh.read(2) == b\"\\x1f\\x8b\"\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# --- EPSS download ---\n",
        "def download_epss_month(year: int, month: int):\n",
        "    \"\"\"Download daily EPSS CSV files for a given month.\"\"\"\n",
        "    days = calendar.monthrange(year, month)[1]\n",
        "    for day in range(1, days + 1):\n",
        "        dstr = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
        "        url = f\"https://epss.empiricalsecurity.com/epss_scores-{dstr}.csv.gz\"\n",
        "        gz_path = EPSS_DIR / f\"epss_{dstr}.csv.gz\"\n",
        "        csv_path = EPSS_DIR / f\"epss_{dstr}.csv\"\n",
        "\n",
        "        print(f\"[EPSS] {url}\")\n",
        "        try:\n",
        "            http_download(url, gz_path, headers=UA)\n",
        "        except Exception as e:\n",
        "            print(f\"  ! Download error: {e}  (skipping)\")\n",
        "            continue\n",
        "\n",
        "        if not is_gzip(gz_path):\n",
        "            bad = gz_path.with_suffix(\".html\")\n",
        "            gz_path.replace(bad)\n",
        "            print(f\"  ! Not a gzip (possibly 404/403). Saved as: {bad.name}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with gzip.open(gz_path, \"rb\") as f_in, open(csv_path, \"wb\") as f_out:\n",
        "                shutil.copyfileobj(f_in, f_out)\n",
        "            gz_path.unlink(missing_ok=True)\n",
        "            print(f\"  ✓ Done: {csv_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ! Gzip error: {e}  (skipping)\")\n",
        "\n",
        "\n",
        "# --- Normalization helpers ---\n",
        "def _norm_cols(cols):\n",
        "    \"\"\"Normalize column names (lowercase, no BOM, underscores instead of spaces).\"\"\"\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        c = (c or \"\").replace(\"\\ufeff\", \"\").strip().lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
        "        out.append(c)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _alias(name):\n",
        "    \"\"\"Map common alternative column names to standard EPSS schema.\"\"\"\n",
        "    if name in {\"cve\", \"cve_id\", \"cveid\", \"cve_identifier\"}: return \"cve\"\n",
        "    if name in {\"epss\", \"epss_score\", \"score\", \"prob\", \"probability\"}: return \"epss\"\n",
        "    if name in {\"percentile\", \"epss_percentile\"}: return \"percentile\"\n",
        "    return name\n",
        "\n",
        "\n",
        "def _read_epss_any(path: Path):\n",
        "    \"\"\"\n",
        "    Robust EPSS CSV reader and normalizer.\n",
        "\n",
        "    - Skips metadata rows (#model_version, score_date) using comment=\"#\"\n",
        "    - Handles BOM, mixed delimiters (, ; or tab)\n",
        "    - Returns a DataFrame with at least 'cve' and 'epss' columns\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
        "        sample = f.read(8192)\n",
        "    if sample.lstrip().startswith(\"<\"):\n",
        "        raise ValueError(f\"File appears to be HTML (bad download?): {path.name}\")\n",
        "\n",
        "    try:\n",
        "        delim = csv.Sniffer().sniff(sample, delimiters=\",;\\t\").delimiter\n",
        "    except Exception:\n",
        "        delim = \",\"\n",
        "\n",
        "    df = pd.read_csv(path, sep=delim, encoding=\"utf-8-sig\", dtype=str, comment=\"#\")\n",
        "\n",
        "    df.columns = _norm_cols(df.columns)\n",
        "    df = df.rename(columns={c: _alias(c) for c in df.columns})\n",
        "\n",
        "    if \"cve\" not in df.columns or \"epss\" not in df.columns:\n",
        "        df2 = pd.read_csv(path, sep=delim, encoding=\"utf-8-sig\", dtype=str, header=None, comment=\"#\")\n",
        "        if len(df2) > 0:\n",
        "            headers = _norm_cols([str(x) for x in df2.iloc[0].tolist()])\n",
        "            headers = [_alias(h) for h in headers]\n",
        "            df2 = df2.drop(index=0).reset_index(drop=True)\n",
        "            df2.columns = headers\n",
        "            df = df2\n",
        "\n",
        "    if \"cve\" not in df.columns or \"epss\" not in df.columns:\n",
        "        raise ValueError(f\"Missing required columns: {path.name} | columns: {df.columns.tolist()}\")\n",
        "\n",
        "    keep = [\"cve\", \"epss\"] + ([\"percentile\"] if \"percentile\" in df.columns else [])\n",
        "    df = df[keep].copy()\n",
        "    df[\"epss\"] = pd.to_numeric(df[\"epss\"], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- Cleaning ---\n",
        "def clean_epss_month():\n",
        "    \"\"\"Read, normalize, and clean all downloaded daily EPSS CSVs.\"\"\"\n",
        "    files = sorted(EPSS_DIR.glob(\"epss_*.csv\"))\n",
        "    if not files:\n",
        "        print(\"[EPSS] Nothing to clean (no CSV files found).\")\n",
        "        return\n",
        "    ok = 0\n",
        "    for f in files:\n",
        "        try:\n",
        "            df = _read_epss_any(f)\n",
        "            out = EPSS_CLEAN / f.name\n",
        "            df[[\"cve\", \"epss\"]].to_csv(out, index=False, encoding=\"utf-8\")\n",
        "            ok += 1\n",
        "            print(f\"[EPSS] CLEAN → {out.name} | rows: {len(df)} | NaN epss: {df['epss'].isna().sum()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[EPSS] CLEAN ERROR → {f.name} | {e}\")\n",
        "    print(f\"[EPSS] Cleaning complete | success: {ok} / {len(files)}\")\n",
        "\n",
        "\n",
        "# --- Aggregation ---\n",
        "def build_month_aggregate():\n",
        "    \"\"\"Merge all cleaned daily EPSS files into a single monthly dataset (day, cve, epss).\"\"\"\n",
        "    import pandas as pd\n",
        "    rows = []\n",
        "    files = sorted(EPSS_CLEAN.glob(\"epss_*.csv\"))\n",
        "    for f in files:\n",
        "        day = f.stem.split(\"_\", 1)[1]\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            df.insert(0, \"day\", day)\n",
        "            rows.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"[EPSS] Skipped merging: {f.name} ({e})\")\n",
        "    if not rows:\n",
        "        print(\"[EPSS] No cleaned files found to merge.\")\n",
        "        return\n",
        "    big = pd.concat(rows, ignore_index=True)\n",
        "    out_csv = BASE_DIR / f\"epss_{YEAR}-{MONTH:02d}_clean_all.csv\"\n",
        "    big.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "    try:\n",
        "        out_parq = BASE_DIR / f\"epss_{YEAR}-{MONTH:02d}_clean_all.parquet\"\n",
        "        big.to_parquet(out_parq, index=False)\n",
        "        print(f\"[EPSS] Monthly aggregate saved: {out_csv.name} and {out_parq.name} | rows: {len(big)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[EPSS] Parquet skipped (pyarrow missing?): {e}\")\n",
        "        print(f\"[EPSS] Monthly CSV saved: {out_csv} | rows: {len(big)}\")\n",
        "\n",
        "\n",
        "# --- Quick preview ---\n",
        "def preview_clean_sample(n=5):\n",
        "    \"\"\"Display a small sample of cleaned EPSS data.\"\"\"\n",
        "    samples = sorted(EPSS_CLEAN.glob(\"epss_*.csv\"))\n",
        "    if not samples:\n",
        "        print(\"[EPSS] No cleaned files available for preview.\")\n",
        "        return\n",
        "    p = samples[0]\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(p, nrows=n)\n",
        "    print(f\"[PREVIEW] {p.name}\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "\n",
        "# --- Main ---\n",
        "def main():\n",
        "    ensure_dirs()\n",
        "    print(f\"==> Working directory: {BASE_DIR.resolve()}\")\n",
        "    download_epss_month(YEAR, MONTH)\n",
        "    clean_epss_month()\n",
        "    build_month_aggregate()\n",
        "    preview_clean_sample(5)\n",
        "    print(\"✅ Done (EPSS downloaded, cleaned, and aggregated by month)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srrqshoUQf_7"
      },
      "source": [
        "Drive include for download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIISzQYOPxbo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Local directory containing the generated results\n",
        "LOCAL_DIR = Path(\"/content/august/battlefields\")\n",
        "\n",
        "# Target directory on Google Drive\n",
        "TARGET_DIR = Path(\"/content/drive/MyDrive/TDK/battlefields/august2\")\n",
        "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Recursively copy the entire directory (Python 3.8+)\n",
        "shutil.copytree(LOCAL_DIR, TARGET_DIR, dirs_exist_ok=True)\n",
        "\n",
        "print(\"✅ Files successfully copied to:\", TARGET_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jkjbTd2e2kp"
      },
      "source": [
        "CVSS download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1iAk0JjWCeW"
      },
      "outputs": [],
      "source": [
        "# Set your API key as an ENVIRONMENT variable (do NOT hardcode it!)\n",
        "%env NVD_API_KEY=YOUR_API_KEY\n",
        "\n",
        "# (Optional) target directory: Google Drive or Colab local filesystem\n",
        "import os\n",
        "os.environ[\"BASE_DIR\"] = \"/content/august\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXleJo32WIS3"
      },
      "outputs": [],
      "source": [
        "import os, json, time, math, urllib.parse\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import requests\n",
        "\n",
        "# ---- CONFIGURATION ----\n",
        "BASE_DIR = Path(os.getenv(\"BASE_DIR\", \"/content/august\"))\n",
        "NVD_DIR  = BASE_DIR / \"nvd\"\n",
        "NVD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "NVD_BASE = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n",
        "\n",
        "# Primary and fallback page sizes\n",
        "RESULTS_PER_PAGE_PRIMARY = 1000   # usually more stable than 2000\n",
        "RESULTS_PER_PAGE_FALLBACK = 200\n",
        "SLEEP_BETWEEN_CALLS_SEC = 1.2\n",
        "\n",
        "# Optional API key (from environment)\n",
        "API_KEY = os.getenv(\"NVD_API_KEY\", \"\").strip()\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Colab TDK downloader)\",\n",
        "    \"Accept\": \"application/json\",\n",
        "    # The API key is intentionally NOT placed here — it will be passed as a query parameter.\n",
        "}\n",
        "\n",
        "\n",
        "# ---- UTILITIES ----\n",
        "def _iso_utc_ms(dt: datetime) -> str:\n",
        "    \"\"\"Return ISO-8601 timestamp with milliseconds and trailing 'Z'.\"\"\"\n",
        "    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
        "\n",
        "\n",
        "def to_iso_date_only(ts: str | None) -> str | None:\n",
        "    \"\"\"Convert a timestamp to a YYYY-MM-DD string (UTC).\"\"\"\n",
        "    if not ts:\n",
        "        return None\n",
        "    s = ts.replace(\"Z\", \"+00:00\")\n",
        "    try:\n",
        "        d = datetime.fromisoformat(s)\n",
        "        return d.date().isoformat()\n",
        "    except Exception:\n",
        "        return ts[:10] if len(ts) >= 10 and ts[4] == \"-\" and ts[7] == \"-\" else None\n",
        "\n",
        "\n",
        "def extract_vendors_from_config(configs) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract affected vendor/brand names from CPE 2.3 URIs.\n",
        "    Handles both 'configurations' dictionaries and lists.\n",
        "    \"\"\"\n",
        "    vendors = set()\n",
        "\n",
        "    def parse_cpe_vendor(cpe: str | None) -> str | None:\n",
        "        # Example: cpe:2.3:a:microsoft:windows_10:...\n",
        "        if not cpe or not isinstance(cpe, str) or not cpe.startswith(\"cpe:2.3:\"):\n",
        "            return None\n",
        "        parts = cpe.split(\":\")\n",
        "        if len(parts) >= 5:\n",
        "            v = parts[3].strip()\n",
        "            return v if v and v != \"*\" else None\n",
        "        return None\n",
        "\n",
        "    def walk_node(node: dict):\n",
        "        for m in (node.get(\"cpeMatch\") or []):\n",
        "            if m.get(\"vulnerable\", True):\n",
        "                cpe = m.get(\"criteria\") or m.get(\"cpe23Uri\") or \"\"\n",
        "                v = parse_cpe_vendor(cpe)\n",
        "                if v:\n",
        "                    vendors.add(v)\n",
        "        for m in (node.get(\"matches\") or []):\n",
        "            cpe = m.get(\"cpeName\") or m.get(\"cpe23Uri\") or m.get(\"criteria\") or \"\"\n",
        "            v = parse_cpe_vendor(cpe)\n",
        "            if v:\n",
        "                vendors.add(v)\n",
        "        for child in (node.get(\"children\") or []):\n",
        "            walk_node(child)\n",
        "        for child in (node.get(\"nodes\") or []):\n",
        "            walk_node(child)\n",
        "\n",
        "    if not configs:\n",
        "        return []\n",
        "\n",
        "    config_items = []\n",
        "    if isinstance(configs, dict):\n",
        "        config_items = [configs]\n",
        "    elif isinstance(configs, list):\n",
        "        config_items = [c for c in configs if isinstance(c, dict)]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    for cfg in config_items:\n",
        "        for n in (cfg.get(\"nodes\") or []):\n",
        "            walk_node(n)\n",
        "\n",
        "    return sorted(vendors)\n",
        "\n",
        "\n",
        "def _mask_url(url: str) -> str:\n",
        "    \"\"\"Hide the API key in log output.\"\"\"\n",
        "    try:\n",
        "        parts = urllib.parse.urlsplit(url)\n",
        "        q = urllib.parse.parse_qs(parts.query, keep_blank_values=True)\n",
        "        if \"apiKey\" in q:\n",
        "            q[\"apiKey\"] = [\"***\"]\n",
        "        new_query = urllib.parse.urlencode(q, doseq=True)\n",
        "        return urllib.parse.urlunsplit((parts.scheme, parts.netloc, parts.path, new_query, parts.fragment))\n",
        "    except Exception:\n",
        "        return url\n",
        "\n",
        "\n",
        "# ---- HTTP HELPERS ----\n",
        "def _do_get(params: dict, use_key: bool, start_index: int, results_per_page: int | None = None):\n",
        "    p = dict(params)\n",
        "    p[\"startIndex\"] = start_index\n",
        "    if results_per_page:\n",
        "        p[\"resultsPerPage\"] = results_per_page\n",
        "    if use_key and API_KEY:\n",
        "        p[\"apiKey\"] = API_KEY  # key passed as query param\n",
        "    r = requests.get(NVD_BASE, params=p, headers=HEADERS, timeout=120)\n",
        "    safe_url = _mask_url(r.url)\n",
        "    return r, safe_url\n",
        "\n",
        "\n",
        "# ---- NVD FETCH ----\n",
        "def fetch_nvd_month(year: int, month: int) -> list[Path]:\n",
        "    \"\"\"Download all NVD CVE records for a given month as JSON pages and return file paths.\"\"\"\n",
        "    # Define publication date range [pubStartDate, pubEndDate)\n",
        "    if month == 12:\n",
        "        start = datetime(year, 12, 1, tzinfo=timezone.utc)\n",
        "        end   = datetime(year + 1, 1, 1, tzinfo=timezone.utc)\n",
        "    else:\n",
        "        start = datetime(year, month, 1, tzinfo=timezone.utc)\n",
        "        end   = datetime(year, month + 1, 1, tzinfo=timezone.utc)\n",
        "\n",
        "    base_pub = {\n",
        "        \"pubStartDate\": _iso_utc_ms(start),\n",
        "        \"pubEndDate\":   _iso_utc_ms(end),\n",
        "    }\n",
        "    print(f\"[NVD] Publication range: {base_pub['pubStartDate']} … {base_pub['pubEndDate']}\")\n",
        "\n",
        "    # (1) First attempt: with API key and large page size\n",
        "    r, dbg_url = _do_get(base_pub, use_key=True, start_index=0, results_per_page=RESULTS_PER_PAGE_PRIMARY)\n",
        "    print(f\"  -> GET {dbg_url} | HTTP {r.status_code}\")\n",
        "\n",
        "    # (2) If 404/403/429/503: retry without key\n",
        "    if r.status_code in (404, 403, 429, 503):\n",
        "        print(\"  ! Error or 404. Retrying WITHOUT API key…\")\n",
        "        r, dbg_url = _do_get(base_pub, use_key=False, start_index=0, results_per_page=RESULTS_PER_PAGE_PRIMARY)\n",
        "        print(f\"  -> GET {dbg_url} | HTTP {r.status_code}\")\n",
        "\n",
        "    # (3) If still 404: fallback to lastMod* parameters\n",
        "    if r.status_code == 404:\n",
        "        base_mod = {\n",
        "            \"lastModStartDate\": base_pub[\"pubStartDate\"],\n",
        "            \"lastModEndDate\":   base_pub[\"pubEndDate\"],\n",
        "        }\n",
        "        print(\"  ! Still 404. Falling back to lastMod* parameters…\")\n",
        "        r, dbg_url = _do_get(base_mod, use_key=False, start_index=0, results_per_page=RESULTS_PER_PAGE_PRIMARY)\n",
        "        print(f\"  -> GET {dbg_url} | HTTP {r.status_code}\")\n",
        "        base_params_in_use = base_mod\n",
        "    else:\n",
        "        base_params_in_use = base_pub\n",
        "\n",
        "    # (4) If not 200: smaller page size and retry once\n",
        "    if r.status_code != 200:\n",
        "        print(\"  ! Still failing. Retrying with smaller resultsPerPage…\")\n",
        "        time.sleep(1.5)\n",
        "        r, dbg_url = _do_get(base_params_in_use, use_key=False, start_index=0, results_per_page=RESULTS_PER_PAGE_FALLBACK)\n",
        "        print(f\"  -> GET {dbg_url} | HTTP {r.status_code}\")\n",
        "        if r.status_code != 200:\n",
        "            print(f\"  ! HTTP {r.status_code}: {r.text[:400]}\")\n",
        "            r.raise_for_status()\n",
        "\n",
        "    data = r.json()\n",
        "    total = int(data.get(\"totalResults\", 0))\n",
        "    got   = len(data.get(\"vulnerabilities\", []) or [])\n",
        "    page_size_used = int(data.get(\"resultsPerPage\", RESULTS_PER_PAGE_PRIMARY))\n",
        "    pages = math.ceil(total / page_size_used) if page_size_used else 0\n",
        "    print(f\"  ✓ Total results: {total} | first page: {got} | expected pages: {pages}\")\n",
        "\n",
        "    out_files = []\n",
        "    out0 = NVD_DIR / f\"nvd_{year:04d}-{month:02d}_p0.json\"\n",
        "    out0.write_text(r.text, encoding=\"utf-8\")\n",
        "    out_files.append(out0)\n",
        "\n",
        "    # Pagination – increment using actual page size returned\n",
        "    start_index = got\n",
        "    page = 1\n",
        "    while start_index < total:\n",
        "        time.sleep(SLEEP_BETWEEN_CALLS_SEC)\n",
        "        rr, dbg_url = _do_get(base_params_in_use, use_key=False, start_index=start_index, results_per_page=RESULTS_PER_PAGE_PRIMARY)\n",
        "        if rr.status_code != 200:\n",
        "            print(f\"  ! Page #{page} HTTP {rr.status_code} – retrying with smaller page size…\")\n",
        "            time.sleep(1.5)\n",
        "            rr, dbg_url = _do_get(base_params_in_use, use_key=False, start_index=start_index, results_per_page=RESULTS_PER_PAGE_FALLBACK)\n",
        "            if rr.status_code != 200:\n",
        "                print(f\"    !! HTTP {rr.status_code}: {rr.text[:200]}\")\n",
        "                break\n",
        "\n",
        "        outp = NVD_DIR / f\"nvd_{year:04d}-{month:02d}_p{page}.json\"\n",
        "        outp.write_text(rr.text, encoding=\"utf-8\")\n",
        "        got = len((rr.json().get(\"vulnerabilities\") or []))\n",
        "        print(f\"  ✓ Page #{page}: {got} records → {outp.name}\")\n",
        "        if got == 0:\n",
        "            break\n",
        "        out_files.append(outp)\n",
        "        start_index += got\n",
        "        page += 1\n",
        "\n",
        "    return out_files\n",
        "\n",
        "\n",
        "# ---- CVSS EXTRACTION ----\n",
        "def build_cvss_with_published_and_vendor(nvd_json_files: list[Path], out_csv: Path):\n",
        "    \"\"\"Parse downloaded NVD JSON files and extract (CVE, published, vendor, CVSS).\"\"\"\n",
        "    import pandas as pd\n",
        "    recs = []\n",
        "    for jf in sorted(nvd_json_files):\n",
        "        data = json.loads(jf.read_text(encoding=\"utf-8\"))\n",
        "        for v in (data.get(\"vulnerabilities\") or []):\n",
        "            cve_id = v[\"cve\"][\"id\"]\n",
        "            published = to_iso_date_only(v[\"cve\"].get(\"published\"))\n",
        "            vendors = extract_vendors_from_config(v[\"cve\"].get(\"configurations\", {}))\n",
        "            vendor_str = \";\".join(vendors) if vendors else None\n",
        "            # CVSS selection: prefer v3.1 > v3.0 > v2 (highest baseScore)\n",
        "            m = v[\"cve\"].get(\"metrics\", {})\n",
        "            best = None\n",
        "            for key in (\"cvssMetricV31\", \"cvssMetricV30\", \"cvssMetricV2\"):\n",
        "                for mm in (m.get(key) or []):\n",
        "                    bs = mm.get(\"cvssData\", {}).get(\"baseScore\")\n",
        "                    if bs is not None:\n",
        "                        val = float(bs)\n",
        "                        best = val if best is None else max(best, val)\n",
        "            if best is not None:\n",
        "                recs.append((cve_id, published, vendor_str, best))\n",
        "\n",
        "    import pandas as pd\n",
        "    (pd.DataFrame(recs, columns=[\"cve\", \"published\", \"vendor\", \"cvss\"])\n",
        "       .drop_duplicates(\"cve\")\n",
        "       .sort_values([\"published\", \"cve\"], na_position=\"last\")\n",
        "       .to_csv(out_csv, index=False, encoding=\"utf-8\"))\n",
        "    print(f\"[OK] CSV saved: {out_csv}  (columns: cve, published, vendor, cvss)\")\n",
        "\n",
        "\n",
        "# ---- MAIN ----\n",
        "def main():\n",
        "    files = fetch_nvd_month(2025, 8)\n",
        "    build_cvss_with_published_and_vendor(files, BASE_DIR / \"nvd_cvss_2025-08.csv\")\n",
        "    print(\"✅ Done (NVD JSONs saved in 'nvd/' and combined output: nvd_cvss_2025-08.csv)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGtfMOB8km3s"
      },
      "source": [
        "Combine EPSS and CVSS in one file per day:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32XCKyozlcgB"
      },
      "outputs": [],
      "source": [
        "%env END_DATE=2025-08-31\n",
        "%env RUN_WHOLE_MONTH=True\n",
        "%env INCLUDE_PREVIOUS_MONTHS=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHtkdOBjlKBO"
      },
      "outputs": [],
      "source": [
        "# build_top10_cumulative_until.py\n",
        "from pathlib import Path\n",
        "import os, re, calendar\n",
        "import pandas as pd\n",
        "\n",
        "# --- SETTINGS / SWITCHES ---\n",
        "BASE_DIR = Path(os.getenv(\"BASE_DIR\", \"/content/august\"))\n",
        "EPSS_DIR = BASE_DIR / \"epss_clean\"\n",
        "OUT_DIR  = BASE_DIR / \"battlefields\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# End date (inclusive)\n",
        "END_DATE_STR = os.getenv(\"END_DATE\", \"2025-08-31\")\n",
        "END_DATE = pd.to_datetime(END_DATE_STR)\n",
        "\n",
        "# If True: automatically load ALL available nvd_cvss_YYYY-MM.csv files up to END_DATE’s month\n",
        "INCLUDE_PREVIOUS_MONTHS = os.getenv(\"INCLUDE_PREVIOUS_MONTHS\", \"False\").lower() in (\"1\", \"true\", \"y\", \"yes\")\n",
        "\n",
        "# NEW SWITCH: if True, generate a cumulative report for each day from the 1st of the month up to END_DATE\n",
        "RUN_WHOLE_MONTH = os.getenv(\"RUN_WHOLE_MONTH\", \"False\").lower() in (\"1\", \"true\", \"y\", \"yes\")\n",
        "\n",
        "\n",
        "# --- Helpers ---\n",
        "def _month_key_from_name(path: Path):\n",
        "    m = re.match(r\"^nvd_cvss_(\\d{4}-\\d{2})\\.csv$\", path.name)\n",
        "    if not m:\n",
        "        return None\n",
        "    y_m = m.group(1)\n",
        "    try:\n",
        "        return pd.to_datetime(y_m + \"-01\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _load_nvd_all_until(end_date: pd.Timestamp) -> pd.DataFrame:\n",
        "    \"\"\"Load and concatenate all relevant NVD monthly CSVs up to the given end_date month.\"\"\"\n",
        "    nvd_files = []\n",
        "    if INCLUDE_PREVIOUS_MONTHS:\n",
        "        for p in BASE_DIR.glob(\"nvd_cvss_*.csv\"):\n",
        "            mk = _month_key_from_name(p)\n",
        "            if mk is not None and mk <= end_date.replace(day=1):\n",
        "                nvd_files.append((mk, p))\n",
        "        nvd_files.sort()\n",
        "    else:\n",
        "        ym = end_date.strftime(\"%Y-%m\")\n",
        "        one = BASE_DIR / f\"nvd_cvss_{ym}.csv\"\n",
        "        assert one.exists(), f\"Missing NVD monthly CSV: {one}\"\n",
        "        nvd_files.append((end_date.replace(day=1), one))\n",
        "\n",
        "    assert nvd_files, \"No nvd_cvss_YYYY-MM.csv files found for the given period.\"\n",
        "\n",
        "    frames = []\n",
        "    for _, fp in nvd_files:\n",
        "        df = pd.read_csv(fp, dtype={\"cve\": str, \"published\": str, \"vendor\": str, \"cvss\": str})\n",
        "        frames.append(df)\n",
        "\n",
        "    nvd_all = pd.concat(frames, ignore_index=True)\n",
        "    nvd_all[\"cve\"] = nvd_all[\"cve\"].str.strip()\n",
        "    nvd_all[\"published\"] = pd.to_datetime(nvd_all[\"published\"], errors=\"coerce\")\n",
        "    nvd_all[\"cvss\"] = pd.to_numeric(nvd_all[\"cvss\"], errors=\"coerce\")\n",
        "    nvd_all = nvd_all.dropna(subset=[\"published\", \"cvss\"])\n",
        "\n",
        "    # Deduplicate CVEs (some appear in multiple monthly dumps)\n",
        "    nvd_all = nvd_all.drop_duplicates(subset=[\"cve\"], keep=\"first\")\n",
        "\n",
        "    # Split and normalize vendor names\n",
        "    nvd_all[\"vendor\"] = nvd_all[\"vendor\"].fillna(\"\")\n",
        "    nvd_all = nvd_all[nvd_all[\"vendor\"] != \"\"].copy()\n",
        "    nvd_all[\"vendor\"] = nvd_all[\"vendor\"].str.split(\";\")\n",
        "    nvd_all = nvd_all.explode(\"vendor\", ignore_index=True)\n",
        "    nvd_all[\"vendor\"] = nvd_all[\"vendor\"].str.strip().str.lower()\n",
        "\n",
        "    return nvd_all\n",
        "\n",
        "\n",
        "def _compute_and_save_for_date(date_str: str, nvd_all: pd.DataFrame) -> None:\n",
        "    \"\"\"Compute cumulative Top-10 vendors up to the given date.\n",
        "       The EPSS snapshot corresponds to the specific day’s EPSS data.\"\"\"\n",
        "    cutoff = pd.to_datetime(date_str)\n",
        "\n",
        "    # EPSS daily file\n",
        "    epss_path = EPSS_DIR / f\"epss_{date_str}.csv\"\n",
        "    if not epss_path.exists():\n",
        "        print(f\"[SKIP] No EPSS file found for this day: {epss_path.name}\")\n",
        "        return\n",
        "\n",
        "    epss = pd.read_csv(epss_path, dtype={\"cve\": str, \"epss\": str})\n",
        "    epss[\"cve\"] = epss[\"cve\"].str.strip()\n",
        "    epss[\"epss\"] = pd.to_numeric(epss[\"epss\"], errors=\"coerce\")\n",
        "    epss = epss.dropna(subset=[\"epss\"])\n",
        "    if epss.empty:\n",
        "        print(f\"[SKIP] Empty EPSS file on this day: {date_str}\")\n",
        "        return\n",
        "\n",
        "    # NVD filtered up to cutoff\n",
        "    nvd_cut = nvd_all[(nvd_all[\"published\"].notna()) & (nvd_all[\"published\"] <= cutoff)].copy()\n",
        "    if nvd_cut.empty:\n",
        "        print(f\"[INFO] Empty NVD slice <= {date_str}\")\n",
        "        out_path = OUT_DIR / f\"top10_vendors_until_{date_str}.csv\"\n",
        "        pd.DataFrame(columns=[\"date\", \"vendor\", \"battlefield_value\", \"n_cves\", \"bf_value_x1000\"]).to_csv(out_path, index=False)\n",
        "        print(f\"[OK] Empty result saved: {out_path}\")\n",
        "        return\n",
        "\n",
        "    # Merge + compute combined value\n",
        "    df = nvd_cut.merge(epss[[\"cve\", \"epss\"]], on=\"cve\", how=\"inner\")\n",
        "    if df.empty:\n",
        "        print(f\"[INFO] No matching CVEs for this day: {date_str}\")\n",
        "        out_path = OUT_DIR / f\"top10_vendors_until_{date_str}.csv\"\n",
        "        pd.DataFrame(columns=[\n",
        "            \"date\", \"vendor\", \"battlefield_value\", \"cvss_sum\", \"epss_sum\",\n",
        "            \"cvss_avg\", \"epss_avg\", \"n_cves\", \"bf_value_x1000\"\n",
        "        ]).to_csv(out_path, index=False)\n",
        "        print(f\"[OK] Empty result saved: {out_path}\")\n",
        "        return\n",
        "\n",
        "    # Value calculation\n",
        "    df[\"cvss\"] = pd.to_numeric(df[\"cvss\"], errors=\"coerce\")\n",
        "    df[\"epss\"] = pd.to_numeric(df[\"epss\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"cvss\", \"epss\"])\n",
        "    df[\"value\"] = df[\"cvss\"] * df[\"epss\"]\n",
        "\n",
        "    # Aggregate per vendor\n",
        "    agg = (\n",
        "        df.groupby(\"vendor\", as_index=False)\n",
        "          .agg(\n",
        "              battlefield_value=(\"value\", \"sum\"),\n",
        "              cvss_sum=(\"cvss\", \"sum\"),\n",
        "              epss_sum=(\"epss\", \"sum\"),\n",
        "              cvss_avg=(\"cvss\", \"mean\"),\n",
        "              epss_avg=(\"epss\", \"mean\"),\n",
        "              n_cves=(\"cve\", \"nunique\"),\n",
        "          )\n",
        "          .sort_values(\"battlefield_value\", ascending=False)\n",
        "          .head(10)\n",
        "    )\n",
        "\n",
        "    # Optional scaling for readability\n",
        "    SCALE = 1000.0\n",
        "    agg[\"bf_value_x1000\"] = agg[\"battlefield_value\"] * SCALE\n",
        "    agg.insert(0, \"date\", date_str)\n",
        "\n",
        "    out_path = OUT_DIR / f\"top10_vendors_until_{date_str}.csv\"\n",
        "    agg.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"[OK] Saved: {out_path}  | rows: {len(agg)}\")\n",
        "\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Load all relevant NVD monthly data once\n",
        "    nvd_all = _load_nvd_all_until(END_DATE)\n",
        "    print(f\"[NVD] Total records loaded: {len(nvd_all)}\")\n",
        "\n",
        "    if RUN_WHOLE_MONTH:\n",
        "        # Iterate from the 1st of the month until END_DATE\n",
        "        month_start = END_DATE.replace(day=1)\n",
        "        for d in pd.date_range(month_start, END_DATE, freq=\"D\"):\n",
        "            _compute_and_save_for_date(d.strftime(\"%Y-%m-%d\"), nvd_all)\n",
        "    else:\n",
        "        # Generate report only for END_DATE\n",
        "        _compute_and_save_for_date(END_DATE_STR, nvd_all)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_IYrqbFe77H"
      },
      "source": [
        "Simulation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCDIXU-A55tv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhWdusQSe7sP"
      },
      "outputs": [],
      "source": [
        "%env BASE_DIR=/content/drive/MyDrive/TDK/battlefields/january\n",
        "%env START_DATE=2025-01-01\n",
        "%env END_DATE=2025-01-31\n",
        "%env EPISODES_PER_DAY=1000\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile /content/rl_value_calendar_fixed_per_day.py\n",
        "# rl_value_calendar_fixed_per_day.py\n",
        "import os, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "# -------------------- ENV helpers (comment-robust) --------------------\n",
        "def env_str(name, default=\"\"):\n",
        "    s = os.getenv(name, default)\n",
        "    return s.split(\"#\", 1)[0].strip()\n",
        "\n",
        "def env_int(name, default):\n",
        "    s = env_str(name, str(default))\n",
        "    return int(s)\n",
        "\n",
        "def env_bool(name, default=False):\n",
        "    s = env_str(name, str(default))\n",
        "    return s.lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
        "\n",
        "def env_float(name, default):\n",
        "    s = env_str(name, str(default))\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        return float(default)\n",
        "\n",
        "\n",
        "# -------------------- Parameters --------------------\n",
        "BASE_DIR = Path(env_str(\"BASE_DIR\", \"/content/august\"))\n",
        "BATTLEFIELD_DIR = BASE_DIR\n",
        "PREFER_CUMULATIVE = env_bool(\"PREFER_CUMULATIVE\", True)  # True → prefer top10_vendors_until_*.csv first\n",
        "START_DATE_STR = env_str(\"START_DATE\", \"\")               # empty → start from earliest available\n",
        "END_DATE_STR   = env_str(\"END_DATE\", \"2025-08-31\")\n",
        "EPISODES_PER_DAY = env_int(\"EPISODES_PER_DAY\", 500)      # number of episodes per day\n",
        "\n",
        "ATTACKER_RESOURCES = 20\n",
        "DEFENDER_RESOURCES = 40\n",
        "\n",
        "# RL (epsilon-greedy bandit)\n",
        "EPSILON_ATTACKER = 0.1\n",
        "EPSILON_DEFENDER = 0.1\n",
        "ALPHA_ATTACKER = 0.1\n",
        "ALPHA_DEFENDER = 0.1\n",
        "\n",
        "# ---------- Stabilization metrics ----------\n",
        "STAB_WINDOW = 1000       # evaluation window (episodes)\n",
        "DOMINANCE_P = 0.70       # dominance threshold (e.g., 70%)\n",
        "Q_TOL = 1e-2             # max allowed |ΔQ| variation\n",
        "CHECK_EVERY = 100        # check every N episodes\n",
        "REQUIRED_STREAK = 2      # need this many consecutive “OK” intervals\n",
        "\n",
        "stabilization_episode_att = None\n",
        "stabilization_episode_def = None\n",
        "streak_ok_att = 0\n",
        "streak_ok_def = 0\n",
        "Q_att_prev_check = None\n",
        "Q_def_prev_check = None\n",
        "\n",
        "# Controlled randomness / outcome noise\n",
        "ATTEMPT_FAIL_PROB = 0.30\n",
        "USE_ADAPTIVE_TARGETING = False\n",
        "TARGET_ATTEMPT_SUCCESS = 0.70\n",
        "ADAPT_EVERY = 200\n",
        "ADAPT_ETA = 0.01\n",
        "\n",
        "# -------------------- Normalization settings --------------------\n",
        "# NORMALIZE_MODE: \"minmax\" | \"sum1\" | \"none\"\n",
        "NORMALIZE_MODE = env_str(\"NORMALIZE_MODE\", \"minmax\").lower()\n",
        "NORMALIZE_EPS = 1e-9\n",
        "WRITE_NORMALIZED_CSV = env_bool(\"WRITE_NORMALIZED_CSV\", True)\n",
        "\n",
        "# The normalized weights are scaled so that their sum equals this target value (default: 10).\n",
        "# If 0 → zero vector; if negative → no constraint.\n",
        "NORMALIZE_SUM_TARGET = env_float(\"NORMALIZE_SUM_TARGET\", 10.0)\n",
        "\n",
        "# -------------------- Scenarios / information visibility --------------------\n",
        "# R0 (baseline): A sees EPSS ✓, D sees EPSS ✓, A sees D strat ✗, D sees A strat ✗\n",
        "# R1 (blue intel edge): A ✗, D ✓, A ✗, D ✓\n",
        "# R2 (red intel edge):  A ✓, D ✗, A ✗, D ✗\n",
        "# R3 (asymmetric):      A ✓, D ✓, A ✗, D ✓\n",
        "SCENARIO = env_str(\"SCENARIO\", \"R1\").upper()\n",
        "\n",
        "# Defaults based on scenario\n",
        "if SCENARIO == \"R1\":\n",
        "    A_SEES_EPSS_DEFAULT = False\n",
        "    D_SEES_EPSS_DEFAULT = True\n",
        "    A_SEES_DEF_STRAT_DEFAULT = False\n",
        "    D_SEES_ATT_STRAT_DEFAULT = True\n",
        "elif SCENARIO == \"R2\":\n",
        "    A_SEES_EPSS_DEFAULT = True\n",
        "    D_SEES_EPSS_DEFAULT = False\n",
        "    A_SEES_DEF_STRAT_DEFAULT = False\n",
        "    D_SEES_ATT_STRAT_DEFAULT = False\n",
        "elif SCENARIO == \"R3\":\n",
        "    A_SEES_EPSS_DEFAULT = True\n",
        "    D_SEES_EPSS_DEFAULT = True\n",
        "    A_SEES_DEF_STRAT_DEFAULT = False\n",
        "    D_SEES_ATT_STRAT_DEFAULT = True\n",
        "else:  # R0\n",
        "    A_SEES_EPSS_DEFAULT = True\n",
        "    D_SEES_EPSS_DEFAULT = True\n",
        "    A_SEES_DEF_STRAT_DEFAULT = False\n",
        "    D_SEES_ATT_STRAT_DEFAULT = False\n",
        "\n",
        "# Overridable via ENV\n",
        "ATTACKER_SEES_EPSS = env_bool(\"ATTACKER_SEES_EPSS\", A_SEES_EPSS_DEFAULT)\n",
        "DEFENDER_SEES_EPSS = env_bool(\"DEFENDER_SEES_EPSS\", D_SEES_EPSS_DEFAULT)\n",
        "ATTACKER_SEES_DEF_STRAT = env_bool(\"ATTACKER_SEES_DEF_STRAT\", A_SEES_DEF_STRAT_DEFAULT)\n",
        "DEFENDER_SEES_ATTACK_STRAT = env_bool(\"DEFENDER_SEES_ATTACK_STRAT\", D_SEES_ATT_STRAT_DEFAULT)\n",
        "\n",
        "# CVSS-proxy flattening parameter (0<p<=1, smaller → flatter → information disadvantage)\n",
        "CVSS_FLATTEN_P = env_float(\"CVSS_FLATTEN_P\", 0.5)\n",
        "\n",
        "\n",
        "# -------------------- Top-k coverage --------------------\n",
        "TOP_K = 3  # can be adjusted as needed\n",
        "\n",
        "def compute_topk_coverage(true_vals, att_all, def_all, k=TOP_K):\n",
        "    if true_vals is None or att_all is None or def_all is None:\n",
        "        return np.nan, np.nan\n",
        "    k = max(1, min(k, len(true_vals)))\n",
        "    top_idx = np.argsort(true_vals)[-k:]\n",
        "    total_top_value = float(np.sum(true_vals[top_idx]))\n",
        "    if total_top_value <= 0:\n",
        "        return np.nan, np.nan\n",
        "    att_mask = (att_all[top_idx] > def_all[top_idx])\n",
        "    def_mask = ~att_mask\n",
        "    att_value = float(np.sum(true_vals[top_idx] * att_mask))\n",
        "    def_value = float(np.sum(true_vals[top_idx] * def_mask))\n",
        "    return att_value / total_top_value, def_value / total_top_value\n",
        "\n",
        "\n",
        "# ---- Dynamic entropy (mixed strategy diversity) ----\n",
        "ENT_WINDOW = 50\n",
        "entropy_att_hist = []\n",
        "entropy_def_hist = []\n",
        "\n",
        "def calc_entropy(counts):\n",
        "    probs = counts / np.sum(counts)\n",
        "    probs = probs[probs > 0]\n",
        "    return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "\n",
        "# -------------------- Battlefield file handling --------------------\n",
        "def _parse_date_from_name(name: str):\n",
        "    # top10_vendors_until_YYYY-MM-DD.csv  OR  top10_vendors_YYYY-MM-DD.csv\n",
        "    m = re.search(r\"top10_vendors_(?:until_)?(\\d{4}-\\d{2}-\\d{2})\\.csv$\", name)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def _list_available_dates(bf_dir: Path) -> list[str]:\n",
        "    if not bf_dir.exists():\n",
        "        return []\n",
        "    dates = set()\n",
        "    for p in bf_dir.glob(\"top10_vendors*.csv\"):\n",
        "        d = _parse_date_from_name(p.name)\n",
        "        if d:\n",
        "            dates.add(d)\n",
        "    return sorted(dates)\n",
        "\n",
        "def _load_values_for_date(date_str: str, prefer_cumulative: bool = True):\n",
        "    \"\"\"\n",
        "    Returns: (values: np.ndarray [N], vendors: list[str], path: Path)\n",
        "    Source: top10_vendors_until_YYYY-MM-DD.csv or top10_vendors_YYYY-MM-DD.csv\n",
        "    \"\"\"\n",
        "    cand = []\n",
        "    if prefer_cumulative:\n",
        "        cand.append(BATTLEFIELD_DIR / f\"top10_vendors_until_{date_str}.csv\")\n",
        "        cand.append(BATTLEFIELD_DIR / f\"top10_vendors_{date_str}.csv\")\n",
        "    else:\n",
        "        cand.append(BATTLEFIELD_DIR / f\"top10_vendors_{date_str}.csv\")\n",
        "        cand.append(BATTLEFIELD_DIR / f\"top10_vendors_until_{date_str}.csv\")\n",
        "\n",
        "    path = next((p for p in cand if p.exists()), None)\n",
        "    if path is None:\n",
        "        raise FileNotFoundError(f\"No battlefield file for {date_str} ({cand[0].name} / {cand[1].name})\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    assert \"vendor\" in cols_lower and \"battlefield_value\" in cols_lower, \\\n",
        "        f\"Missing columns in file: {path.name} (required: vendor, battlefield_value)\"\n",
        "\n",
        "    df = df.rename(columns={cols_lower[\"vendor\"]: \"vendor\", cols_lower[\"battlefield_value\"]: \"battlefield_value\"})\n",
        "    df = df.sort_values(\"battlefield_value\", ascending=False).head(10).reset_index(drop=True)\n",
        "    vendors = df[\"vendor\"].astype(str).tolist()\n",
        "    vals = pd.to_numeric(df[\"battlefield_value\"], errors=\"coerce\").fillna(0.0).to_numpy(dtype=float)\n",
        "    return vals, vendors, path\n",
        "\n",
        "# -------------------- Normalization functions --------------------\n",
        "def _normalize_values(raw: np.ndarray, mode: str, sum_target: float) -> np.ndarray:\n",
        "    mode = (mode or \"minmax\").lower()\n",
        "    x = np.asarray(raw, dtype=float)\n",
        "\n",
        "    # Protect against empty input\n",
        "    if x.size == 0:\n",
        "        return x.copy()\n",
        "\n",
        "    # (1) base normalization\n",
        "    if mode == \"none\":\n",
        "        norm = x.copy()\n",
        "    elif mode == \"sum1\":\n",
        "        s = x.sum()\n",
        "        if s < NORMALIZE_EPS:\n",
        "            norm = np.zeros_like(x, dtype=float)\n",
        "        else:\n",
        "            norm = x / (s + NORMALIZE_EPS)\n",
        "    else:  # default: min-max\n",
        "        mn, mx = float(np.min(x)), float(np.max(x))\n",
        "        rng = mx - mn\n",
        "        if rng < NORMALIZE_EPS:\n",
        "            norm = np.zeros_like(x, dtype=float)\n",
        "        else:\n",
        "            norm = (x - mn) / (rng + NORMALIZE_EPS)\n",
        "\n",
        "    # (2) enforce total-sum constraint (if sum_target >= 0)\n",
        "    if sum_target >= 0:\n",
        "        cur_sum = float(np.sum(norm))\n",
        "        if cur_sum < NORMALIZE_EPS or sum_target == 0:\n",
        "            norm = np.zeros_like(norm)\n",
        "        else:\n",
        "            norm = norm * (sum_target / cur_sum)\n",
        "\n",
        "    return norm\n",
        "\n",
        "\n",
        "def _cvss_proxy_from_raw(raw: np.ndarray, sum_target: float, p: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    CVSS proxy: less informative, flatter weights.\n",
        "    Steps:\n",
        "      1) min-max normalization to 0..1\n",
        "      2) raise to power p (0<p<=1) → flattening\n",
        "      3) sum1 normalization and scaling to target sum\n",
        "    \"\"\"\n",
        "    x = np.asarray(raw, dtype=float)\n",
        "    if x.size == 0:\n",
        "        return x.copy()\n",
        "    mn, mx = float(np.min(x)), float(np.max(x))\n",
        "    rng = mx - mn\n",
        "    if rng < NORMALIZE_EPS:\n",
        "        flat = np.ones_like(x, dtype=float)\n",
        "    else:\n",
        "        flat = (x - mn) / (rng + NORMALIZE_EPS)\n",
        "    flat = np.power(np.clip(flat, 0.0, 1.0), max(NORMALIZE_EPS, min(1.0, p)))  # 0<p<=1\n",
        "    s = flat.sum()\n",
        "    if s < NORMALIZE_EPS:\n",
        "        flat[:] = 0.0\n",
        "    else:\n",
        "        flat = flat / s\n",
        "    if sum_target >= 0:\n",
        "        flat = flat * sum_target\n",
        "    return flat\n",
        "\n",
        "\n",
        "# -------------------- Assemble daily battlefield list --------------------\n",
        "all_dates = _list_available_dates(BATTLEFIELD_DIR)\n",
        "assert all_dates, f\"No battlefield CSVs found in: {BATTLEFIELD_DIR}\"\n",
        "\n",
        "if START_DATE_STR:\n",
        "    all_dates = [d for d in all_dates if d >= START_DATE_STR]\n",
        "all_dates = [d for d in all_dates if d <= END_DATE_STR]\n",
        "assert all_dates, f\"No available dates within range (START_DATE={START_DATE_STR or 'AUTO'}, END_DATE={END_DATE_STR}).\"\n",
        "\n",
        "print(f\"[DEBUG] Available days in range: {len(all_dates)} ({all_dates[0]} … {all_dates[-1]})\")\n",
        "print(f\"[CFG] EPISODES_PER_DAY={EPISODES_PER_DAY}, PREFER_CUMULATIVE={PREFER_CUMULATIVE}\")\n",
        "print(f\"[CFG] FIXED mode | ATTEMPT_FAIL_PROB={ATTEMPT_FAIL_PROB}, USE_ADAPTIVE_TARGETING={USE_ADAPTIVE_TARGETING}\")\n",
        "print(f\"[CFG] NORMALIZE_MODE={NORMALIZE_MODE}, WRITE_NORMALIZED_CSV={WRITE_NORMALIZED_CSV}, NORMALIZE_SUM_TARGET={NORMALIZE_SUM_TARGET}\")\n",
        "print(f\"[CFG] SCENARIO={SCENARIO} | A_SEES_EPSS={ATTACKER_SEES_EPSS}, D_SEES_EPSS={DEFENDER_SEES_EPSS}, \"\n",
        "      f\"A_SEES_DEF_STRAT={ATTACKER_SEES_DEF_STRAT}, D_SEES_ATT_STRAT={DEFENDER_SEES_ATTACK_STRAT}, \"\n",
        "      f\"CVSS_FLATTEN_P={CVSS_FLATTEN_P}\")\n",
        "\n",
        "\n",
        "# -------------------- Strategy helpers (value-based) --------------------\n",
        "def normalize_and_allocate(weights, total):\n",
        "    w = np.array(weights, dtype=float)\n",
        "    w = np.maximum(w, 1e-9)\n",
        "    w = w / w.sum()\n",
        "    raw = w * total\n",
        "    alloc = np.floor(raw).astype(int)\n",
        "    rem = total - alloc.sum()\n",
        "    if rem > 0:\n",
        "        frac = raw - alloc\n",
        "        for i in np.argsort(frac)[-rem:]:\n",
        "            alloc[i] += 1\n",
        "    return alloc\n",
        "\n",
        "def top_k_targeting(vals, total, k):\n",
        "    k = max(1, min(k, len(vals)))\n",
        "    alloc = np.zeros(len(vals), dtype=int)\n",
        "    idx = np.argsort(vals)[-k:]\n",
        "    base = total // k\n",
        "    rem = total % k\n",
        "    alloc[idx] = base\n",
        "    if rem > 0:\n",
        "        alloc[idx[:rem]] += 1\n",
        "    return alloc\n",
        "\n",
        "def all_in_top1(vals, total):\n",
        "    alloc = np.zeros(len(vals), dtype=int)\n",
        "    alloc[int(np.argmax(vals))] = total\n",
        "    return alloc\n",
        "\n",
        "def strat_even(total, n):               return normalize_and_allocate(np.ones(n), total)\n",
        "def strat_vals(total, vals):            return normalize_and_allocate(vals, total)\n",
        "def strat_top3(total, vals):            return top_k_targeting(vals, total, k=3)\n",
        "def strat_random(total, n):             return normalize_and_allocate(np.random.rand(n), total)\n",
        "\n",
        "def attacker_counter(total, last_def_alloc, fallback_alloc):\n",
        "    if last_def_alloc is None:\n",
        "        return fallback_alloc\n",
        "    # if the attacker sees defender strategy (by default false), react; otherwise fallback\n",
        "    if not ATTACKER_SEES_DEF_STRAT:\n",
        "        return fallback_alloc\n",
        "    inv = last_def_alloc.max() - last_def_alloc + 1\n",
        "    return normalize_and_allocate(inv, total)\n",
        "\n",
        "def defender_counter(total, last_att_alloc, fallback_alloc):\n",
        "    if last_att_alloc is None:\n",
        "        return fallback_alloc\n",
        "    if not DEFENDER_SEES_ATTACK_STRAT:\n",
        "        return fallback_alloc\n",
        "    return normalize_and_allocate(last_att_alloc + 1, total)\n",
        "# -------------------- Daily switching and visibility handling --------------------\n",
        "vendors_names = []\n",
        "raw_values = None\n",
        "values_epss = None        # EPSS-like (informative) scale — primary “values”\n",
        "values_cvss_proxy = None  # CVSS-proxy (flatter)\n",
        "NUM_BATTLEFIELDS = 0\n",
        "last_att_alloc = None\n",
        "last_def_alloc = None\n",
        "\n",
        "# Day-specific planning vectors (visibility-dependent)\n",
        "att_planning_values = None\n",
        "def_planning_values  = None\n",
        "\n",
        "def set_day(date_str: str):\n",
        "    global values_epss, values_cvss_proxy, NUM_BATTLEFIELDS\n",
        "    global last_att_alloc, last_def_alloc, vendors_names, raw_values\n",
        "    global att_planning_values, def_planning_values\n",
        "    global attack_history_sum\n",
        "\n",
        "    vals, vendors, src = _load_values_for_date(date_str, prefer_cumulative=PREFER_CUMULATIVE)\n",
        "    raw_values = vals.copy()\n",
        "    vendors_names = vendors\n",
        "\n",
        "    # EPSS-informed scale (the “true” battlefield values)\n",
        "    values_epss = _normalize_values(raw_values, NORMALIZE_MODE, NORMALIZE_SUM_TARGET)\n",
        "\n",
        "    # CVSS-proxy – less informative, flattened weights\n",
        "    values_cvss_proxy = _cvss_proxy_from_raw(raw_values, NORMALIZE_SUM_TARGET, CVSS_FLATTEN_P)\n",
        "\n",
        "    NUM_BATTLEFIELDS = len(values_epss)\n",
        "    last_att_alloc = None\n",
        "    last_def_alloc = None\n",
        "\n",
        "    # Reset attack pattern history for this day\n",
        "    attack_history_sum = np.zeros(NUM_BATTLEFIELDS, dtype=float)\n",
        "\n",
        "    print(f\"[DAY] {date_str} | battlefields: {NUM_BATTLEFIELDS} | source: {src.name}\")\n",
        "    if NUM_BATTLEFIELDS == 0:\n",
        "        print(f\"[SKIP] No battlefields on {date_str} — skipping day.\")\n",
        "        return False\n",
        "\n",
        "    # Visibility-based planning vectors\n",
        "    att_planning_values = values_epss if ATTACKER_SEES_EPSS else values_cvss_proxy\n",
        "    def_planning_values = values_epss if DEFENDER_SEES_EPSS else values_cvss_proxy\n",
        "    return True\n",
        "\n",
        "\n",
        "# -------------------- Strategy pools (value-based) --------------------\n",
        "# Strategies are built upon visibility-dependent planning vectors.\n",
        "\n",
        "# --- Attacker strategies ---\n",
        "attacker_strategies = [\n",
        "    (\"A1 Even\",            lambda: strat_even(ATTACKER_RESOURCES, NUM_BATTLEFIELDS)),\n",
        "    (\"A2 Value-weighted\",  lambda: strat_vals(ATTACKER_RESOURCES, att_planning_values)),\n",
        "    (\"A3 Top-3 targeting\", lambda: strat_top3(ATTACKER_RESOURCES, att_planning_values)),\n",
        "    (\"A4 Randomized\",      lambda: strat_random(ATTACKER_RESOURCES, NUM_BATTLEFIELDS)),\n",
        "    (\"A5 All-in (Top-1)\",  lambda: all_in_top1(att_planning_values, ATTACKER_RESOURCES)),\n",
        "]\n",
        "\n",
        "# --- Attack pattern memory ---\n",
        "attack_history_sum = np.zeros(10, dtype=float)  # assuming 10 battlefields\n",
        "attack_history_decay = 0.9                      # exponential decay for past influence\n",
        "\n",
        "def defender_predictive_counter(total_resources, last_att_alloc, fallback_alloc):\n",
        "    \"\"\"\n",
        "    Predictive defense strategy:\n",
        "      - uses exponential decay over past attack patterns,\n",
        "      - protects frequently targeted “hot” fields more strongly,\n",
        "      - blends prior EPSS-based risk,\n",
        "      - adds slight noise to avoid deterministic freezing.\n",
        "    \"\"\"\n",
        "    global attack_history_sum, values_epss\n",
        "\n",
        "    if last_att_alloc is None:\n",
        "        return fallback_alloc\n",
        "\n",
        "    # (1) Update attack history with decay\n",
        "    attack_history_sum = attack_history_decay * attack_history_sum + last_att_alloc\n",
        "\n",
        "    # (2) Normalize historical frequency\n",
        "    hist_norm = attack_history_sum / (np.sum(attack_history_sum) + 1e-9)\n",
        "\n",
        "    # (3) Combine with EPSS prior (if visible)\n",
        "    epss_norm = values_epss / (np.sum(values_epss) + 1e-9)\n",
        "    combined_pred = 0.7 * hist_norm + 0.3 * epss_norm  # ratio adjustable\n",
        "\n",
        "    # (4) Add small random noise\n",
        "    noise = np.random.rand(len(combined_pred)) * 0.02\n",
        "    combined_pred = np.clip(combined_pred + noise, 0, None)\n",
        "\n",
        "    # (5) Emphasize strong activity zones (non-linear weighting)\n",
        "    combined_pred = np.power(combined_pred, 1.5)\n",
        "\n",
        "    # (6) Normalize and allocate\n",
        "    return normalize_and_allocate(combined_pred, total_resources)\n",
        "\n",
        "\n",
        "# --- Defender strategies ---\n",
        "defender_strategies = [\n",
        "    (\"D1 Even\",             lambda: strat_even(DEFENDER_RESOURCES, NUM_BATTLEFIELDS)),\n",
        "    (\"D2 Value-weighted\",   lambda: strat_vals(DEFENDER_RESOURCES, def_planning_values)),\n",
        "    (\"D3 Top-3 defense\",    lambda: strat_top3(DEFENDER_RESOURCES, def_planning_values)),\n",
        "    (\"D4 Randomized\",       lambda: strat_random(DEFENDER_RESOURCES, NUM_BATTLEFIELDS)),\n",
        "    (\"D5 Predictive\",       None),\n",
        "    (\"D6 All-in (Top-1)\",   lambda: all_in_top1(def_planning_values, DEFENDER_RESOURCES)),\n",
        "]\n",
        "\n",
        "# --- Enable or disable D5 depending on visibility ---\n",
        "if DEFENDER_SEES_ATTACK_STRAT:\n",
        "    for i, (name, fn) in enumerate(defender_strategies):\n",
        "        if name.startswith(\"D5\"):\n",
        "            defender_strategies[i] = (\n",
        "                name,\n",
        "                lambda: defender_predictive_counter(\n",
        "                    DEFENDER_RESOURCES,\n",
        "                    last_att_alloc,\n",
        "                    strat_vals(DEFENDER_RESOURCES, def_planning_values)\n",
        "                )\n",
        "            )\n",
        "else:\n",
        "    # remove D5 if attacker strategy is invisible\n",
        "    defender_strategies = [s for s in defender_strategies if not s[0].startswith(\"D5\")]\n",
        "\n",
        "\n",
        "# -------------------- Initialize Q-tables and metrics --------------------\n",
        "NA = len(attacker_strategies)\n",
        "ND = len(defender_strategies)\n",
        "\n",
        "Q_att = np.zeros(NA)\n",
        "Q_def = np.zeros(ND)\n",
        "\n",
        "att_used, def_used = [], []\n",
        "att_rewards, def_rewards = [], []\n",
        "win_flags = []\n",
        "\n",
        "last_att_alloc = None\n",
        "last_def_alloc = None\n",
        "\n",
        "total_attempts = 0\n",
        "total_successes = 0\n",
        "\n",
        "def attack_attempt_succeeds(att_tokens, def_tokens):\n",
        "    \"\"\"Probabilistic attack success based on ATTEMPT_FAIL_PROB.\"\"\"\n",
        "    p_success = float(np.clip(1.0 - ATTEMPT_FAIL_PROB, 0.0, 1.0))\n",
        "    return np.random.rand() < p_success\n",
        "\n",
        "\n",
        "# -------------------- Main loop: run EPISODES_PER_DAY for each day --------------------\n",
        "total_episodes = 0\n",
        "\n",
        "for day_str in all_dates:\n",
        "    ok = set_day(day_str)\n",
        "    if not ok or NUM_BATTLEFIELDS == 0:\n",
        "        continue  # skip day if no battlefields available\n",
        "\n",
        "    for ep in range(EPISODES_PER_DAY):\n",
        "        # epsilon-greedy selection\n",
        "        a_idx = np.random.randint(NA) if np.random.rand() < EPSILON_ATTACKER else int(np.argmax(Q_att))\n",
        "        d_idx = np.random.randint(ND) if np.random.rand() < EPSILON_DEFENDER else int(np.argmax(Q_def))\n",
        "\n",
        "        # allocations\n",
        "        att_alloc = attacker_strategies[a_idx][1]()\n",
        "        if defender_strategies[d_idx][0].startswith(\"D7\"):\n",
        "            fallback = strat_vals(DEFENDER_RESOURCES, def_planning_values)\n",
        "            def_alloc = defender_counter(DEFENDER_RESOURCES, last_att_alloc, fallback)\n",
        "        else:\n",
        "            def_alloc = defender_strategies[d_idx][1]()\n",
        "\n",
        "        # compute payoffs using EPSS-scale as “true” battlefield value\n",
        "        true_values = values_epss\n",
        "        att_reward_real = 0.0\n",
        "        def_reward = 0.0\n",
        "        any_breakthrough = False\n",
        "        att_access_reward = 0.0\n",
        "\n",
        "        for i in range(len(true_values)):\n",
        "            if att_alloc[i] > def_alloc[i]:\n",
        "                att_access_reward = max(att_access_reward, true_values[i])\n",
        "                total_attempts += 1\n",
        "                if attack_attempt_succeeds(att_alloc[i], def_alloc[i]):\n",
        "                    total_successes += 1\n",
        "                    any_breakthrough = True\n",
        "                    att_reward_real = max(att_reward_real, true_values[i])\n",
        "                else:\n",
        "                    def_reward += true_values[i]\n",
        "            else:\n",
        "                def_reward += true_values[i]\n",
        "\n",
        "        # Q-learning updates\n",
        "        Q_att[a_idx] += ALPHA_ATTACKER * (att_access_reward - Q_att[a_idx])\n",
        "        Q_def[d_idx] += ALPHA_DEFENDER * (def_reward - Q_def[d_idx])\n",
        "\n",
        "        # record history and metrics\n",
        "        last_att_alloc = att_alloc\n",
        "        last_def_alloc = def_alloc\n",
        "        att_used.append(a_idx); def_used.append(d_idx)\n",
        "        att_rewards.append(att_reward_real)\n",
        "        def_rewards.append(def_reward)\n",
        "        win_flags.append(1 if any_breakthrough else 0)\n",
        "\n",
        "        # optional adaptive calibration\n",
        "        if USE_ADAPTIVE_TARGETING and (total_episodes + 1) % ADAPT_EVERY == 0 and total_attempts > 0:\n",
        "            observed = total_successes / total_attempts\n",
        "            ATTEMPT_FAIL_PROB = float(np.clip(\n",
        "                ATTEMPT_FAIL_PROB + ADAPT_ETA * (observed - TARGET_ATTEMPT_SUCCESS),\n",
        "                0.0, 1.0\n",
        "            ))\n",
        "            total_attempts = 0\n",
        "            total_successes = 0\n",
        "            print(f\"[ADAPT] New ATTEMPT_FAIL_PROB={ATTEMPT_FAIL_PROB:.3f} (observed={observed:.3f})\")\n",
        "\n",
        "        # --- STABILIZATION CHECKS (attacker & defender) ---\n",
        "        if (total_episodes) % CHECK_EVERY == 0:\n",
        "            # --- Attacker ---\n",
        "            if len(att_used) >= STAB_WINDOW and stabilization_episode_att is None:\n",
        "                last_actions_att = np.array(att_used[-STAB_WINDOW:])\n",
        "                counts_att = np.bincount(last_actions_att, minlength=NA)\n",
        "                dom_idx_att = counts_att.argmax()\n",
        "                dom_share_att = counts_att.max() / STAB_WINDOW\n",
        "                dom_ok_att = (dom_share_att >= DOMINANCE_P) and (np.argmax(Q_att) == dom_idx_att)\n",
        "\n",
        "                if Q_att_prev_check is None:\n",
        "                    q_ok_att = False\n",
        "                else:\n",
        "                    q_drift_att = np.max(np.abs(Q_att - Q_att_prev_check))\n",
        "                    q_ok_att = (q_drift_att < Q_TOL)\n",
        "                Q_att_prev_check = Q_att.copy()\n",
        "\n",
        "                if dom_ok_att and q_ok_att:\n",
        "                    streak_ok_att += 1\n",
        "                else:\n",
        "                    streak_ok_att = 0\n",
        "                if streak_ok_att >= REQUIRED_STREAK:\n",
        "                    stabilization_episode_att = total_episodes\n",
        "\n",
        "            # --- Defender ---\n",
        "            if len(def_used) >= STAB_WINDOW and stabilization_episode_def is None:\n",
        "                last_actions_def = np.array(def_used[-STAB_WINDOW:])\n",
        "                counts_def = np.bincount(last_actions_def, minlength=ND)\n",
        "                dom_idx_def = counts_def.argmax()\n",
        "                dom_share_def = counts_def.max() / STAB_WINDOW\n",
        "                dom_ok_def = (dom_share_def >= DOMINANCE_P) and (np.argmax(Q_def) == dom_idx_def)\n",
        "\n",
        "                if Q_def_prev_check is None:\n",
        "                    q_ok_def = False\n",
        "                else:\n",
        "                    q_drift_def = np.max(np.abs(Q_def - Q_def_prev_check))\n",
        "                    q_ok_def = (q_drift_def < Q_TOL)\n",
        "                Q_def_prev_check = Q_def.copy()\n",
        "\n",
        "                if dom_ok_def and q_ok_def:\n",
        "                    streak_ok_def += 1\n",
        "                else:\n",
        "                    streak_ok_def = 0\n",
        "                if streak_ok_def >= REQUIRED_STREAK:\n",
        "                    stabilization_episode_def = total_episodes\n",
        "\n",
        "            # --- Dynamic entropy (measure of mixed strategy diversity) ---\n",
        "            if len(att_used) >= ENT_WINDOW:\n",
        "                last_actions_att = np.array(att_used[-ENT_WINDOW:])\n",
        "                last_actions_def = np.array(def_used[-ENT_WINDOW:])\n",
        "                counts_att = np.bincount(last_actions_att, minlength=NA)\n",
        "                counts_def = np.bincount(last_actions_def, minlength=ND)\n",
        "                entropy_att_hist.append(calc_entropy(counts_att))\n",
        "                entropy_def_hist.append(calc_entropy(counts_def))\n",
        "\n",
        "        total_episodes += 1\n",
        "\n",
        "\n",
        "# -------------------- Summary metrics --------------------\n",
        "print(\"\\n Key performance metrics (averaged over all episodes):\")\n",
        "print(f\"  Total episodes: {total_episodes}\")\n",
        "print(f\"  Mean attacker reward (realized): {np.mean(att_rewards):.3f}\")\n",
        "print(f\"  Mean defender retained value:    {np.mean(def_rewards):.3f}\")\n",
        "print(f\"  Attacker breakthrough rate:      {np.mean(win_flags)*100:.1f}%\")\n",
        "attempt_rate = 100.0 * (total_successes / max(1, total_attempts))\n",
        "print(f\"  Attacker per-attempt success rate: {attempt_rate:.1f}%\")\n",
        "print(f\"  Learned best attacker strategy: {attacker_strategies[int(np.argmax(Q_att))][0]} (Q={Q_att.max():.3f})\")\n",
        "print(f\"  Learned best defender strategy: {defender_strategies[int(np.argmax(Q_def))][0]} (Q={Q_def.max():.3f})\")\n",
        "\n",
        "\n",
        "# -------------------- Plots --------------------\n",
        "def movavg(x, w=200):\n",
        "    if len(x) < w:\n",
        "        return np.array(x, dtype=float)\n",
        "    k = np.ones(w) / w\n",
        "    return np.convolve(x, k, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(movavg(att_rewards), label='Attacker reward (MA)', alpha=0.9)\n",
        "plt.plot(movavg(def_rewards), label='Defender reward (MA)', alpha=0.9)\n",
        "plt.xlabel(\"Episode\"); plt.ylabel(\"Reward (moving average)\")\n",
        "plt.title(\"Reward convergence – value-based dynamic fields + visibility scenarios\")\n",
        "plt.grid(True); plt.legend(); plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 3.6))\n",
        "bins_a = np.arange(len(attacker_strategies) + 1) - 0.5\n",
        "bins_d = np.arange(len(defender_strategies) + 1) - 0.5\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(att_used, bins=bins_a, edgecolor='black')\n",
        "plt.xticks(range(len(attacker_strategies)), [s[0].split()[0] for s in attacker_strategies], rotation=45)\n",
        "plt.title(\"Attacker strategy frequencies\"); plt.grid(True, axis='y')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(def_used, bins=bins_d, edgecolor='black')\n",
        "plt.xticks(range(len(defender_strategies)), [s[0].split()[0] for s in defender_strategies], rotation=45)\n",
        "plt.title(\"Defender strategy frequencies\"); plt.grid(True, axis='y')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "# -------------------- Q-values & stability summary --------------------\n",
        "print(\"\\n Attacker Q-values:\")\n",
        "for i, (name, _) in enumerate(attacker_strategies):\n",
        "    print(f\"  {name:22s} Q={Q_att[i]:.3f}\")\n",
        "\n",
        "print(\"\\n Defender Q-values:\")\n",
        "for i, (name, _) in enumerate(defender_strategies):\n",
        "    print(f\"  {name:22s} Q={Q_def[i]:.3f}\")\n",
        "\n",
        "att_stab = stabilization_episode_att if stabilization_episode_att is not None else \"N/A (not stabilized)\"\n",
        "def_stab = stabilization_episode_def if stabilization_episode_def is not None else \"N/A (not stabilized)\"\n",
        "\n",
        "print(\"\\n Stabilization episode counts:\")\n",
        "print(f\"  Attacker: {att_stab}\")\n",
        "print(f\"  Defender: {def_stab}\")\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(entropy_att_hist, label='Attacker entropy (H)', alpha=0.9)\n",
        "plt.plot(entropy_def_hist, label='Defender entropy (H)', alpha=0.9)\n",
        "plt.xlabel(\"Episode\"); plt.ylabel(\"Entropy (bits)\")\n",
        "plt.title(\"Strategy distribution entropy – dynamic equilibrium indicator\")\n",
        "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f\"\\nAverage attacker entropy: {np.mean(entropy_att_hist):.3f}\")\n",
        "print(f\"Average defender entropy: {np.mean(entropy_def_hist):.3f}\")\n",
        "\n",
        "# --- Final-day Top-K coverage ---\n",
        "topk_att_cov_final, topk_def_cov_final = compute_topk_coverage(\n",
        "    values_epss, last_att_alloc, last_def_alloc, k=TOP_K\n",
        ")\n",
        "print(f\"\\nTop-{TOP_K} coverage on final day:\")\n",
        "print(f\"  Attacker coverage of top-{TOP_K} values: {topk_att_cov_final*100:.1f}%\")\n",
        "print(f\"  Defender retention of top-{TOP_K} values: {topk_def_cov_final*100:.1f}%\")\n",
        "\n",
        "\n",
        "# -------------------- Summary export function --------------------\n",
        "def main_run():\n",
        "    global total_episodes, att_rewards, def_rewards, win_flags\n",
        "    global entropy_att_hist, entropy_def_hist\n",
        "    global stabilization_episode_att, stabilization_episode_def\n",
        "    global Q_att, Q_def, attacker_strategies, defender_strategies\n",
        "    global values_epss, last_att_alloc, last_def_alloc\n",
        "    global TOP_K, all_dates, SCENARIO\n",
        "\n",
        "    # Top-K coverage on final day\n",
        "    topk_att_cov_final, topk_def_cov_final = compute_topk_coverage(\n",
        "        values_epss, last_att_alloc, last_def_alloc, k=TOP_K\n",
        "    )\n",
        "\n",
        "    stab_att = stabilization_episode_att if stabilization_episode_att is not None else np.nan\n",
        "    stab_def = stabilization_episode_def if stabilization_episode_def is not None else np.nan\n",
        "\n",
        "    q_att_max, q_def_max = float(np.max(Q_att)), float(np.max(Q_def))\n",
        "    q_att_min, q_def_min = float(np.min(Q_att)), float(np.min(Q_def))\n",
        "\n",
        "    ent_att_mean = float(np.mean(entropy_att_hist)) if len(entropy_att_hist) else np.nan\n",
        "    ent_def_mean = float(np.mean(entropy_def_hist)) if len(entropy_def_hist) else np.nan\n",
        "\n",
        "    best_att_idx = int(np.argmax(Q_att))\n",
        "    best_def_idx = int(np.argmax(Q_def))\n",
        "\n",
        "    results = {\n",
        "        \"scenario\": SCENARIO,\n",
        "        \"num_days\": len(all_dates),\n",
        "        \"total_episodes\": total_episodes,\n",
        "        \"att_reward_mean\": float(np.mean(att_rewards)),\n",
        "        \"def_reward_mean\": float(np.mean(def_rewards)),\n",
        "        \"win_rate\": float(np.mean(win_flags)),\n",
        "        \"stab_episode_att\": stab_att,\n",
        "        \"stab_episode_def\": stab_def,\n",
        "        \"entropy_att_mean\": ent_att_mean,\n",
        "        \"entropy_def_mean\": ent_def_mean,\n",
        "        \"entropy_gap\": ent_att_mean -\n",
        "    results = {\n",
        "        # --- core simulation identifiers ---\n",
        "        \"scenario\": SCENARIO,\n",
        "        \"num_days\": len(all_dates),\n",
        "        \"total_episodes\": total_episodes,\n",
        "\n",
        "        # --- performance metrics ---\n",
        "        \"att_reward_mean\": float(np.mean(att_rewards)),\n",
        "        \"def_reward_mean\": float(np.mean(def_rewards)),\n",
        "        \"win_rate\": float(np.mean(win_flags)),\n",
        "\n",
        "        # --- strategic stability and mixing ---\n",
        "        \"stab_episode_att\": stab_att,\n",
        "        \"stab_episode_def\": stab_def,\n",
        "        \"entropy_att_mean\": ent_att_mean,\n",
        "        \"entropy_def_mean\": ent_def_mean,\n",
        "        \"entropy_gap\": (\n",
        "            ent_att_mean - ent_def_mean\n",
        "            if not np.isnan(ent_att_mean) and not np.isnan(ent_def_mean)\n",
        "            else np.nan\n",
        "        ),\n",
        "\n",
        "        # --- learning outcomes ---\n",
        "        \"q_att_max\": q_att_max,\n",
        "        \"q_att_min\": q_att_min,\n",
        "        \"q_def_max\": q_def_max,\n",
        "        \"q_def_min\": q_def_min,\n",
        "        \"best_att_strat\": attacker_strategies[best_att_idx][0],\n",
        "        \"best_def_strat\": defender_strategies[best_def_idx][0],\n",
        "\n",
        "        # --- final-day top-k coverage ---\n",
        "        \"topk_att_cov_final\": float(topk_att_cov_final) if topk_att_cov_final is not None else np.nan,\n",
        "        \"topk_def_cov_final\": float(topk_def_cov_final) if topk_def_cov_final is not None else np.nan,\n",
        "\n",
        "        # --- attacker/defender efficiency ratio ---\n",
        "        \"att_def_reward_ratio\": (\n",
        "            float(np.mean(att_rewards)) / max(1e-9, float(np.mean(def_rewards)))\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main_run()\n",
        "    print(\"\\n=== main_run() summary ===\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k:28s}: {v}\")\n",
        "\n",
        "    # optional: save reward history for plotting\n",
        "    out_dir = Path(\"results\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    pd.DataFrame({\n",
        "        \"episode\": np.arange(len(att_rewards)),\n",
        "        \"att_reward\": att_rewards,\n",
        "        \"def_reward\": def_rewards\n",
        "    }).to_csv(out_dir / f\"reward_history_{SCENARIO}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "PmdpO97FqDj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run for every month:"
      ],
      "metadata": {
        "id": "GrPC8GKw4amy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Multiple deterministic runs for each month and scenario ---\n",
        "import importlib, random, numpy as np, pandas as pd, os\n",
        "\n",
        "# Reload the main simulation script\n",
        "import rl_value_calendar_fixed_per_day as sim\n",
        "\n",
        "# Reproducibility\n",
        "def set_global_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "# Scenarios and months\n",
        "scenarios = [\"R0\", \"R1\", \"R2\", \"R3\"]\n",
        "months = {\n",
        "    \"january\": (\"/content/drive/MyDrive/TDK/battlefields/january\", \"2025-01-01\", \"2025-01-31\"),\n",
        "    \"may\":     (\"/content/drive/MyDrive/TDK/battlefields/may\",     \"2025-05-01\", \"2025-05-31\"),\n",
        "    \"august\":  (\"/content/drive/MyDrive/TDK/battlefields/august\",  \"2025-08-01\", \"2025-08-31\"),\n",
        "}\n",
        "\n",
        "n_runs_per_scenario = 10\n",
        "all_results = []\n",
        "\n",
        "# Main loop\n",
        "for month_name, (base_dir, start, end) in months.items():\n",
        "    for scenario in scenarios:\n",
        "        for run_id in range(1, n_runs_per_scenario + 1):\n",
        "            seed = 1000 * hash(month_name + scenario) % (2**16) + run_id\n",
        "            set_global_seed(seed)\n",
        "\n",
        "            # --- Set environment variables for the current run ---\n",
        "            os.environ[\"BASE_DIR\"] = base_dir\n",
        "            os.environ[\"START_DATE\"] = start\n",
        "            os.environ[\"END_DATE\"] = end\n",
        "            os.environ[\"SCENARIO\"] = scenario\n",
        "\n",
        "            print(f\"\\n=== Run: {month_name} | {scenario} | repetition {run_id}/10 | seed={seed} ===\")\n",
        "\n",
        "            # Reload simulation (fresh import for each run)\n",
        "            importlib.reload(sim)\n",
        "            result = sim.main_run()\n",
        "\n",
        "            # Add identification metadata\n",
        "            result.update({\n",
        "                \"month\": month_name,\n",
        "                \"run_id\": run_id,\n",
        "                \"seed\": seed,\n",
        "            })\n",
        "            all_results.append(result)\n",
        "\n",
        "# --- Save all results to CSV ---\n",
        "df = pd.DataFrame(all_results)\n",
        "csv_path = \"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\"\n",
        "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"\\n✅ All runs completed successfully! Results saved to: {csv_path}\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ItIRRyx3Eydg",
        "outputId": "b6bd2f76-f33c-4103-df4a-edd2b41789a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization:"
      ],
      "metadata": {
        "id": "F0yU5gfboqfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\")\n",
        "\n",
        "# Keep only R0–R3 scenarios\n",
        "df = df[df[\"scenario\"].isin([\"R0\", \"R1\", \"R2\", \"R3\"])]\n",
        "\n",
        "# Average entropy per scenario\n",
        "grouped = df.groupby(\"scenario\", as_index=False)[[\"entropy_att_mean\", \"entropy_def_mean\"]].mean()\n",
        "grouped = grouped.rename(columns={\n",
        "    \"entropy_att_mean\": \"Attacker entropy\",\n",
        "    \"entropy_def_mean\": \"Defender entropy\"\n",
        "})\n",
        "\n",
        "# Plot\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "\n",
        "sns.barplot(\n",
        "    data=grouped.melt(id_vars=\"scenario\", var_name=\"Agent\", value_name=\"Entropy (H)\"),\n",
        "    x=\"scenario\", y=\"Entropy (H)\", hue=\"Agent\",\n",
        "    palette=[\"tab:red\", \"tab:blue\"], edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Scenario\")\n",
        "plt.ylabel(\"Average strategic entropy (bits)\")\n",
        "plt.legend(title=\"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", borderaxespad=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/5.2.pdf\", dpi=300, bbox_inches=\"tight\")  # safety raster version\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EsCzayw7Qq8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\")\n",
        "\n",
        "# Keep only R0–R3 scenarios (in case others are present)\n",
        "df = df[df[\"scenario\"].isin([\"R0\", \"R1\", \"R2\", \"R3\"])]\n",
        "\n",
        "# Compute frequency of the most-learned strategies\n",
        "att_freq = df[\"best_att_strat\"].value_counts(normalize=True).sort_index()\n",
        "def_freq = df[\"best_def_strat\"].value_counts(normalize=True).sort_index()\n",
        "\n",
        "# DataFrame for plotting\n",
        "plot_df = pd.DataFrame({\n",
        "    \"Strategy\": list(att_freq.index) + list(def_freq.index),\n",
        "    \"Frequency\": list(att_freq.values) + list(def_freq.values),\n",
        "    \"Agent\": [\"Attacker\"] * len(att_freq) + [\"Defender\"] * len(def_freq)\n",
        "})\n",
        "\n",
        "# Plot\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "sns.barplot(\n",
        "    data=plot_df,\n",
        "    x=\"Strategy\", y=\"Frequency\", hue=\"Agent\",\n",
        "    palette=[\"tab:red\", \"tab:blue\"], edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Relative frequency\")\n",
        "plt.xlabel(\"Strategy\")\n",
        "plt.legend(title=\"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/5.3.pdf\", dpi=300, bbox_inches=\"tight\")  # safe raster backup\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XWOM4Dg7XCtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === 1. Load CSV ===\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\")\n",
        "\n",
        "# Keep only R0–R3 scenarios\n",
        "df = df[df[\"scenario\"].isin([\"R0\", \"R1\", \"R2\", \"R3\"])]\n",
        "\n",
        "# === 2. Strategy name mapping ===\n",
        "attacker_names = {\n",
        "    \"A1\": \"Even allocation\",\n",
        "    \"A2\": \"Value-proportional\",\n",
        "    \"A3\": \"Top-3 targeting\",\n",
        "    \"A4\": \"Random allocation\",\n",
        "    \"A5\": \"All-in (Top-1)\"\n",
        "}\n",
        "defender_names = {\n",
        "    \"D1\": \"Even allocation\",\n",
        "    \"D2\": \"Value-proportional\",\n",
        "    \"D3\": \"Top-3 defense\",\n",
        "    \"D4\": \"Random allocation\",\n",
        "    \"D5\": \"Predictive counter\",\n",
        "    \"D6\": \"All-in (Top-1)\"\n",
        "}\n",
        "\n",
        "# === 3. Compute relative frequencies ===\n",
        "att_freq = df[\"best_att_strat\"].value_counts(normalize=True)\n",
        "def_freq = df[\"best_def_strat\"].value_counts(normalize=True)\n",
        "\n",
        "# Keep only the three most frequent strategies for each agent\n",
        "att_top3 = att_freq.head(3)\n",
        "def_top3 = def_freq.head(3)\n",
        "\n",
        "# === 4. Prepare DataFrame for plotting (ID + name) ===\n",
        "plot_df = pd.DataFrame({\n",
        "    \"Strategy\": (\n",
        "        [f\"{k} – {attacker_names.get(k, k)}\" for k in att_top3.index] +\n",
        "        [f\"{k} – {defender_names.get(k, k)}\" for k in def_top3.index]\n",
        "    ),\n",
        "    \"Frequency\": list(att_top3.values) + list(def_top3.values),\n",
        "    \"Agent\": [\"Attacker\"] * len(att_top3) + [\"Defender\"] * len(def_top3)\n",
        "})\n",
        "\n",
        "# === 5. Plot ===\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "sns.barplot(\n",
        "    data=plot_df,\n",
        "    x=\"Strategy\", y=\"Frequency\", hue=\"Agent\",\n",
        "    palette=[\"tab:red\", \"tab:blue\"], edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Relative frequency\")\n",
        "plt.xlabel(\"Strategy\")\n",
        "plt.legend(title=\"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/5.3.pdf\", dpi=300, bbox_inches=\"tight\")  # safe raster backup\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1p2tCk-tXn4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\")\n",
        "\n",
        "# Keep only relevant metrics\n",
        "plot_df = df[[\"scenario\", \"topk_att_cov_final\", \"topk_def_cov_final\"]]\n",
        "\n",
        "# Convert to long format\n",
        "melted = plot_df.melt(\n",
        "    id_vars=\"scenario\",\n",
        "    var_name=\"Agent\",\n",
        "    value_name=\"Coverage\"\n",
        ")\n",
        "\n",
        "# Relabel columns\n",
        "melted[\"Agent\"] = melted[\"Agent\"].map({\n",
        "    \"topk_att_cov_final\": \"Attacker\",\n",
        "    \"topk_def_cov_final\": \"Defender\"\n",
        "})\n",
        "\n",
        "# Plot settings\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "sns.barplot(\n",
        "    data=melted,\n",
        "    x=\"scenario\", y=\"Coverage\", hue=\"Agent\",\n",
        "    palette=[\"tab:red\", \"tab:blue\"], edgecolor=\"black\"\n",
        ")\n",
        "\n",
        "plt.ylabel(\"Top-3 coverage\")\n",
        "plt.xlabel(\"Information regime\")\n",
        "plt.ylim(0, 1)  # 0–1 scale (represents 0–100%)\n",
        "plt.legend(title=\"\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/5.4.pdf\", dpi=300, bbox_inches=\"tight\")  # safe raster backup\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uRMWwqiIfi09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/TDK/results/tdk_batch_results.csv\")\n",
        "df[\"rho\"] = df[\"att_reward_mean\"] / df[\"def_reward_mean\"]\n",
        "\n",
        "# Custom, clearly distinguishable colors (especially for R1 and R3)\n",
        "palette = {\n",
        "    \"R0\": \"#1f77b4\",  # blue\n",
        "    \"R1\": \"#e41a1c\",  # bright red\n",
        "    \"R2\": \"#984ea3\",  # deep purple\n",
        "    \"R3\": \"#4daf4a\",  # green\n",
        "}\n",
        "\n",
        "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "sns.pointplot(\n",
        "    data=df,\n",
        "    x=\"month\", y=\"rho\", hue=\"scenario\",\n",
        "    palette=palette, dodge=0.3,\n",
        "    markers=\"o\", linestyles=\"-\", errorbar=(\"sd\")\n",
        ")\n",
        "\n",
        "plt.ylabel(r\"Reward ratio ($\\rho$)\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylim(0, 0.25)\n",
        "plt.legend(title=\"Scenario\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "plt.savefig(\"figures/5.6.png\", dpi=300)\n",
        "plt.savefig(\"figures/5.6.pdf\", dpi=300, bbox_inches=\"tight\")  # safe raster backup\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c_IpLzjrg8gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Data ---\n",
        "scenarios = [\"R0\", \"R1\", \"R2\", \"R3\"]\n",
        "attacker = [1.24, 0.71, 1.41, 0.70]\n",
        "defender = [8.40, 8.98, 8.16, 9.00]\n",
        "ratio = [0.15, 0.08, 0.17, 0.08]\n",
        "\n",
        "x = np.arange(len(scenarios))\n",
        "width = 0.35\n",
        "\n",
        "# --- Dark, print-friendly colors ---\n",
        "color_red = \"tab:red\"    # attacker\n",
        "color_blue = \"tab:blue\"  # defender\n",
        "color_green = \"#1a9850\"  # ratio line\n",
        "\n",
        "# --- Figure setup ---\n",
        "fig, ax1 = plt.subplots(figsize=(7.5, 4))\n",
        "\n",
        "# The 'edgecolor' and 'linewidth' parameters add visible outlines\n",
        "bar_att = ax1.bar(\n",
        "    x - width / 2, attacker, width,\n",
        "    color=color_red, edgecolor=\"black\", linewidth=0.8,\n",
        "    label=\"Attacker ($\\\\bar{R}_A$)\"\n",
        ")\n",
        "bar_def = ax1.bar(\n",
        "    x + width / 2, defender, width,\n",
        "    color=color_blue, edgecolor=\"black\", linewidth=0.8,\n",
        "    label=\"Defender ($\\\\bar{R}_D$)\"\n",
        ")\n",
        "\n",
        "ax1.set_ylabel(r\"Reward ($\\bar{R}_A$, $\\bar{R}_D$)\", fontsize=11)\n",
        "ax1.set_xlabel(\"Scenario\", fontsize=11)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(scenarios, fontsize=10)\n",
        "\n",
        "# Secondary axis (green line for ratio)\n",
        "ax2 = ax1.twinx()\n",
        "line_ratio, = ax2.plot(\n",
        "    x, ratio, marker=\"o\", color=color_green, linewidth=2.2,\n",
        "    label=r\"Reward ratio ($\\rho$)\"\n",
        ")\n",
        "ax2.set_ylabel(r\"Reward ratio ($\\rho$)\", fontsize=11)\n",
        "ax2.set_ylim(0, 0.25)\n",
        "\n",
        "# Legend\n",
        "handles = [bar_att, bar_def, line_ratio]\n",
        "labels = [h.get_label() for h in handles]\n",
        "ax1.legend(\n",
        "    handles, labels,\n",
        "    title=\"Metric\",\n",
        "    bbox_to_anchor=(1.15, 0.9),\n",
        "    loc=\"upper left\",\n",
        "    fontsize=10,\n",
        "    title_fontsize=10,\n",
        "    frameon=False\n",
        ")\n",
        "\n",
        "ax1.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.subplots_adjust(right=0.83, left=0.1, bottom=0.12, top=0.95)\n",
        "\n",
        "# --- Save for thesis ---\n",
        "plt.savefig(\"figures/5.5.pdf\", bbox_inches=\"tight\")   # vector graphic\n",
        "plt.savefig(\"figures/5.5.png\", dpi=300, bbox_inches=\"tight\")  # raster backup\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KiB69Oltku0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpAH0NUZBGHIY5UcUqExTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}